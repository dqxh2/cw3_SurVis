const generatedBibEntries = {
    "balasubramaniam2022transparency": {
        "abstract": "[Context and Motivation] Recent studies have highlighted transparency and explainability as important quality requirements of AI systems. However, there are still relatively few case studies that describe the current state of defining these quality requirements in practice. [Question] The goal of our study was to explore what ethical guidelines organizations have defined for the development of transparent and explainable AI systems. We analyzed the ethical guidelines in 16 organizations representing different industries and public sector. [Results] In the ethical guidelines, the importance of transparency was highlighted by almost all of the organizations, and explainability was considered as an integral part of transparency. Building trust in AI systems was one of the key reasons for developing transparency and explainability, and customers and users were raised as the main target groups of the explanations. The organizations also mentioned developers, partners, and stakeholders as important groups needing explanations. The ethical guidelines contained the following aspects of the AI system that should be explained: the purpose, role of AI, inputs, behavior, data utilized, outputs, and limitations. The guidelines also pointed out that transparency and explainability relate to several other quality requirements, such as trustworthiness, understandability, traceability, privacy, auditability, and fairness. [Contribution] For researchers, this paper provides insights into what organizations consider important in the transparency and, in particular, explainability of AI systems. For practitioners, this study suggests a structured way to define explainability requirements of AI systems.",
        "author": "Balasubramaniam, Nagadivya and Kauppinen, Marjo and Hiekkanen, Kari and Kujala, Sari",
        "booktitle": "International Working Conference on Requirements Engineering: Foundation for Software Quality",
        "keywords": "type:Transparency, Explainability, Quality Requirements, Ethical Guidelines, AI Systems",
        "organization": "Springer",
        "pages": "3--18",
        "title": "Transparency and explainability of AI systems: ethical guidelines in practice",
        "type": "inproceedings",
        "year": "2022"
    },
    "karaccay2024guarding": {
        "abstract": "With the recent advances in 5G and 6G communications and the increasing need for immersive interactions due to pandemic, new use cases such as All-Senses meeting are emerging. To realize these use cases, numerous sensors, actuators, and virtual reality devices are used. Additionally, artificial intelligence (AI) and machine learning (ML) including generative AI can be used to analyze large amount of data generated by 6G networks and devices to enable new applications and services. While AI/ML technologies are evolving, they do not have the same level of security as well-known information technology components. So, AI/ML threats and their impacts can be overlooked. On the other hand, due to inherent characteristics of AI/ML components and design of AI/ML pipeline, AI/ML services can be a target for sophisticated attacks. In order to provide a holistic security view, the effect of AI/ML components should be investigated, threats should be identified, and countermeasures should be planned. Therefore, in this study, which is an extended version of our recent study (Kara\u00e7ay et al. 2023), we shed the light on the use of AI/ML services including generative large language model scenarios in All-Senses meeting use case and their security aspects by carrying out a threat modeling using the STRIDE framework and attack tree methodology. Additionally, we point out some countermeasures for identified threats.",
        "author": "Kara{\\c{c}}ay, Leyli and Laaroussi, Zakaria and Soykan, Elif Ustundag and others",
        "journal": "Annals of Telecommunications",
        "keywords": "6G, AI/ML, threat analysis, type:security, holograp",
        "pages": "1--15",
        "publisher": "Springer",
        "title": "Guarding 6G use cases: a deep dive into AI/ML threats in All-Senses meeting",
        "type": "article",
        "year": "2024"
    },
    "ntoutsi2020bias": {
        "abstract": "Artificial Intelligence (AI)-based systems are widely employed nowadays to make decisions that have far-reaching impact on individuals and society. Their decisions might affect everyone, everywhere, and anytime, entailing concerns about potential human rights issues. Therefore, it is necessary to move beyond traditional AI algorithms optimized for predictive performance and embed ethical and legal principles in their design, training, and deployment to ensure social good while still benefiting from the huge potential of the AI technology. The goal of this survey is to provide a broad multidisciplinary overview of the area of bias in AI systems, focusing on technical challenges and solutions as well as to suggest new research directions towards approaches well-grounded in a legal frame. In this survey, we focus on data-driven AI, as a large part of AI is powered nowadays by (big) data and powerful machine learning algorithms. If otherwise not specified, we use the general term bias to describe problems related to the gathering or processing of data that might result in prejudiced decisions on the bases of demographic features such as race, sex, and so forth.",
        "author": "Ntoutsi, Eirini and Fafalios, Pavlos and Gadiraju, Ujwal and Iosifidis, Vasileios and Nejdl, Wolfgang and Vidal, Maria-Esther and Ruggieri, Salvatore and Turini, Franco and Papadopoulos, Symeon and Krasanakis, Emmanouil and others",
        "journal": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery",
        "keywords": "type:fairness, fairness-aware AI, fairness-aware machine learning, interpretability, responsible AI",
        "number": "3",
        "pages": "e1356",
        "publisher": "Wiley Online Library",
        "title": "Bias in data-driven artificial intelligence systems\u2014An introductory survey",
        "type": "article",
        "volume": "10",
        "year": "2020"
    },
    "nussbaumer2023framework": {
        "abstract": "The development and utilisation of new information and communication technologies presents opportunities and risks, which bring ethical issues to the forefront. Any attempt to minimise the potential negative consequences to individuals, organisations and society resulting from the use of these technologies is challenging. In order to address these challenges, this paper presents an ethics-by-design approach that has been developed and implemented in the context of Decision Supports Systems for Emergency Management. Such systems help manage large and cross-border disasters by supporting decision makers to respond on emergencies in a reasonable way by taking follow-up actions into account. The approach taken in this paper specifically provides means to support the ethical dimensions of these decisions. Actions taken during disasters can have ramifications that persist long after a disaster has passed. The ethics-by-design approach presented here not only informs the design of systems, but also considers the role and training of the decision makers in the design process. The paper builds on the literature on ethics in information systems and makes a contribution to theory by providing a framework to ensure ethical considerations are embedded into the design of systems.",
        "author": "Nussbaumer, Alexander and Pope, Andrew and Neville, Karen",
        "journal": "Information Systems Journal",
        "keywords": "computer ethics, decision support, decision support systems, emergency management, ethics, ethics-by-design,type:ethical frameworks",
        "number": "1",
        "pages": "34--55",
        "publisher": "Wiley Online Library",
        "title": "A framework for applying ethics-by-design to decision support systems for emergency management",
        "type": "article",
        "volume": "33",
        "year": "2023"
    },
    "peters2020responsible": {
        "abstract": "In 2019, the IEEE launched the P7000 standards projects intended to address ethical issues in the design of autonomous and intelligent systems. This move came amidst a growing public concern over the unintended consequences of artificial intelligence (AI), compounded by the lack of an anticipatory process for attending to ethical impact within professional practice. However, the difficulty in moving from principles to practice presents a significant challenge to the implementation of ethical guidelines. Herein, we describe two complementary frameworks for integrating ethical analysis into engineering practice to help address this challenge. We then provide the outcomes of an ethical analysis informed by these frameworks, conducted within the specific context of Internet-delivered therapy in digital mental health. We hope both the frameworks and analysis can provide tools and insights, not only for the context of digital healthcare but also for data-enabled and intelligent technology development more broadly.",
        "author": "Peters, Dorian and Vold, Karina and Robinson, Diana and Calvo, Rafael A",
        "journal": "IEEE Transactions on Technology and Society",
        "keywords": "Digital health, ethics, value-sensitive design(VSD), type:ethical frameworks",
        "number": "1",
        "pages": "34--47",
        "publisher": "IEEE",
        "title": "Responsible AI\u2014two frameworks for ethical design practice",
        "type": "article",
        "volume": "1",
        "year": "2020"
    },
    "shin2020user": {
        "abstract": "With the growing presence of algorithms and their far-reaching effects, artificial intelligence (AI) will be mainstream trends any time soon. Despite this surging popularity, little is known about the processes through which people perceive and make a sense of trust through algorithmic characteristics in a personalized algorithm system. This study examines the extent to which trust can be linked to how perceptions of automated personalization by AI and the processes of such perceptions influence user heuristic and systematic processes. It examines how fair, accountable, transparent, and interpretable people perceive the use of algorithmic recommendations by digital platforms. When users perceive that the algorithm is fairer, more accountable, transparent, and explainable, they see it as more trustworthy and useful. This demonstrates that trust is of particular value to users and further implies the heuristic roles of algorithmic characteristics in terms of their underlying links to trust and subsequent attitudes toward algorithmic decisions. The processes offer a useful perspective on the conceptualization of AI experience and interaction. User cognitive processes identified provide solid foundations for algorithm design and development and a stronger basis for the design of sensemaking AI services.",
        "author": "Shin, Donghee",
        "journal": "Journal of Broadcasting \\& Electronic Media",
        "keywords": "type:transparency, explainability, AI",
        "number": "4",
        "pages": "541--565",
        "publisher": "Taylor \\& Francis",
        "title": "User perceptions of algorithmic decisions in the personalized AI system: Perceptual evaluation of fairness, accountability, transparency, and explainability",
        "type": "article",
        "volume": "64",
        "year": "2020"
    },
    "solanki2023operationalising": {
        "abstract": "Artificial intelligence (AI) offers much promise for improving healthcare. However, it runs the looming risk of causing individual and societal harms; for instance, exacerbating inequalities amongst minority groups, or enabling compromises in the confidentiality of patients\u2019 sensitive data. As such, there is an expanding, unmet need for ensuring AI for healthcare is developed in concordance with human values and ethics. Augmenting \u201cprinciple-based\u201d guidance that highlight adherence to ethical ideals (without necessarily offering translation into actionable practices), we offer a solution-based framework for operationalising ethics in AI for healthcare. Our framework is built from a scoping review of existing solutions of ethical AI guidelines, frameworks and technical solutions to address human values such as self-direction in healthcare. Our view spans the entire length of the AI lifecycle: data management, model development, deployment and monitoring. Our focus in this paper is to collate actionable solutions (whether technical or non-technical in nature), which can be steps that enable and empower developers in their daily practice to ensuring ethical practices in the broader picture. Our framework is intended to be adopted by AI developers, with recommendations that are accessible and driven by the existing literature. We endorse the recognised need for \u2018ethical AI checklists\u2019 co-designed with health AI practitioners, which could further operationalise the technical solutions we have collated. Since the risks to health and wellbeing are so large, we believe a proactive approach is necessary for ensuring human values and ethics are appropriately respected in AI for healthcare.",
        "author": "Solanki, Pravik and Grundy, John and Hussain, Waqar",
        "journal": "AI and Ethics",
        "keywords": "Artifcial intelligence, Machine learning, Healthcare, Medicine, Human values, type:ethical frameworks",
        "number": "1",
        "pages": "223--240",
        "publisher": "Springer",
        "title": "Operationalising ethics in artificial intelligence for healthcare: A framework for AI developers",
        "type": "article",
        "volume": "3",
        "year": "2023"
    },
    "solans2022human": {
        "abstract": " Artificial Intelligence (AI) is increasingly used to build Decision Support Systems (DSS) across many domains. This paper describes a series of experiments designed to observe human response to different characteristics of a DSS such as accuracy and bias, particularly the extent to which participants rely on the DSS, and the performance they achieve. In our experiments, participants play a simple online game inspired by so-called \"wildcat\" (i.e., exploratory) drilling for oil. The landscape has two layers: a visible layer describing the costs (terrain), and a hidden layer describing the reward (oil yield). Participants in the control group play the game without receiving any assistance, while in treatment groups they are assisted by a DSS suggesting places to drill. For certain treatments, the DSS does not consider costs, but only rewards, which introduces a bias that is observable by users. Between subjects, we vary the accuracy and bias of the DSS, and observe the participants' total score, time to completion, the extent to which they follow or ignore suggestions. We also measure the acceptability of the DSS in an exit survey. Our results show that participants tend to score better with the DSS, that the score increase is due to users following the DSS advice, and related to the difficulty of the game and the accuracy of the DSS. We observe that this setting elicits mostly rational behavior from participants, who place a moderate amount of trust in the DSS and show neither algorithmic aversion (under-reliance) nor automation bias (over-reliance).However, their stated willingness to accept the DSS in the exit survey seems less sensitive to the accuracy of the DSS than their behavior, suggesting that users are only partially aware of the (lack of) accuracy of the DSS.",
        "author": "Solans, David and Beretta, Andrea and Portela, Manuel and Castillo, Carlos and Monreale, Anna",
        "doi": "10.1109/ICASSP48485.2024.10446380",
        "journal": "arXiv preprint arXiv:2203.15514",
        "keywords": "type:fairness, Human-AI interaction, AI, DSS",
        "title": "Human Response to an AI-Based Decision Support System: A User Study on the Effects of Accuracy and Bias",
        "type": "article",
        "year": "2022"
    },
    "soykan2022survey": {
        "abstract": "As machine learning and artificial intelligence (ML/AI) are becoming more popular and advanced, there is a wish to turn sensitive data into valuable information via ML/AI techniques revealing only data that is allowed by concerned parties or without revealing any information about the data to third parties. Collaborative ML approaches like federated learning (FL) help tackle these needs and concerns, bringing a way to use sensitive data without disclosing critically sensitive features of that data. In this paper, we provide a detailed analysis of state of the art for collaborative ML approaches from a privacy perspective. A detailed threat model and security and privacy considerations are given for each collaborative method. We deeply analyze Privacy Enhancing Technologies (PETs), covering secure multi-party computation (SMPC), homomorphic encryption (HE), differential privacy (DP), and confidential computing (CC) in the context of collaborative ML. We introduce a guideline on the selection of the privacy preserving technologies for collaborative ML and privacy practitioners. This study constitutes the first survey to provide an in-depth focus on collaborative ML requirements and constraints for privacy solutions while also providing guidelines on the selection of PETs.",
        "author": "Soykan, Elif Ustundag and Karacay, Leyli and Karakoc, Ferhat and Tomur, Emrah",
        "journal": "IEEE Access",
        "keywords": "Collaborative machine learning, privacy enhancing technologies, privacy, type:security, feder\u0002ated learning, artificial intelligence, ML, AI",
        "pages": "97495--97519",
        "publisher": "IEEE",
        "title": "A survey and guideline on privacy enhancing technologies for collaborative machine learning",
        "type": "article",
        "volume": "10",
        "year": "2022"
    },
    "srivastava2019rating": {
        "abstract": "New decision-support systems are being built using AI services that draw insights from a large corpus of data and incorporate those insights in human-in-the-loop decision environments. They promise to transform businesses, such as health care, with better, affordable, and timely decisions. However, it will be unreasonable to expect people to trust AI systems out of the box if they have been shown to exhibit discrimination across a variety of data usages: unstructured text, structured data, or images. Thus, AI systems come with certain risks, such as failing to recognize people or objects, introducing errors in their output, and leading to unintended harm. In response, we propose ratings as a way to communicate bias risk and methods to rate AI services for bias in a black-box fashion without accessing services training data. Our method is designed not only to work on single services, but also the composition of services, which is how complex AI applications are built. Thus, the proposed method can be used to rate a composite application, like a chatbot, for the severity of its bias by rating its constituent services and then composing the rating, rather than rating the whole system.",
        "author": "Srivastava, Biplav and Rossi, Francesca",
        "journal": "IBM Journal of Research and Development",
        "keywords": "type:fairness, human-in-the-loop decision environments, bias",
        "number": "4/5",
        "pages": "5--1",
        "publisher": "IBM",
        "title": "Rating AI systems for bias to promote trustable applications",
        "type": "article",
        "volume": "63",
        "year": "2019"
    }
};